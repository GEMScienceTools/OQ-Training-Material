The Hazard Modeller's Toolkit (or ''oq-hazard-modeller'') is a new development of the GEM OpenQuake suite, which is intended to provide scientists and engineers with the tools to help create the seismogenic input models that go into the OpenQuake hazard engine. The process of developing building the hazard model is a complex and often challenging one, and whilst many aspect of the practice are relatively common, the choice of certain methods or tools for undertaking each step can be a matter of judgement. The intention of this software is to provide scientists and engineers with the means to apply many of the most commonly used algorithms for preparing seismogenic source models using seismicitiy and geological data. It is still in an early stage of development and the current versions contain only a preliminary set of tools for undertaking the necessary workflows. In forthcoming versions will hope to make available more tools for the current processes indicated here, and to integrate new functionalities for i) merging and homogenisation of earthqake catalogues, ii) calculation of activity rates from geological and geodetic data, iii) testing and interpretation of Ground Motion Prediction Equations, and iv) integration of seismological and geological data and treatment of uncertainty in the construction of seismogenic source zones


\section{Getting Started and Running the Software}

The Modeller's Toolkit and associated software are designed for execution from the command line. As with OpenQuake, the preferred environment is Ubuntu Linux (11.04 or later). A careful effort has been made to keep the number of additional dependencies to a minimum. No packaged version of the software has been released at the time of writing, so the user must install the dependencies manually. The online documentation for the Modeller's Toolkit can be found at\\ \href{http://docs.openquake.org/mtoolkit}{http://docs.openquake.org/mtoolkit}.

\textbf{For the purposes of the current exercises all of the Modeller's Toolkit dependencies have been installed in both the OpenQuake .ova files and on the OATS server - so no installation is necessary! For instructions on how to install OpenQuake and the Modeller's Toolkit, please refer to the appendix of this document}

The execution of the Modeller's Toolkit is done from within the main directory in which the code is contained. The selection of the algorithm and the general configuration of the software is done via a configuration file (in this example \verb=config.yml=. This is a YAML (Yet Another Markup Language) file, which can be opened and edited with any common text editor (e.g. vi, emacs, gedit).

To run the Modeller's Toolkit simply move into the directory in which the \verb=main.py= file is found. Once the configuration file is correctly set up, execute via the following command:
\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
~\$ python main.py -i config.yml -d
\end{Verbatim}

The \verb=-d= indicates that the program can be run in ''debug'' mode. This will create a log (written automatically to the file \verb=debug.log=) of the processes that are executed inside the toolkit in addition to certain parameters that may be calculated at intermediate steps of the process, but are not written to the output. The \verb=-d= can be omitted if preferred, in which case the program will return only a minimal indication of the progress to the command prompt.

\subsection{The Configuration File}

The configuration file indicates to the toolkit where to look for input files, write output files, which processes and methods to apply and any possible parameters that should be set for given methods. A ''comprehensive'' example of the configuation file is given automatically in the file \verb=config.yml=, which outlines all possible options. The easiest process is to open this file, then edit and delete various options and parameters before saving the configution file to a new file (e.g. \verb=my_config_1.yml=) then executing that in place of the original configuration file. 

The various settings for the different algorithms will be discussed in due course, so the following considers just the main headers (i.e. the control) of the toolkit:

\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize, samepage=true]
\# *********************************************************
\# MT Workflow configuration file
\# *********************************************************
\\
\# =========================================================
\# Input/Output files
\# =========================================================
\\
\# Path to the file defining the eq catalog.
eq_catalog_file: tests/data/an_input_catalogue.csv
\\
\# Path to the file defining the transformed
\# eq catalog after the preprocessing jobs.
\# If not defined no file will be written.
pprocessing_result_file: tests/data/preprocessed_catalogue.csv
\\
\# Path to the file defining the computed
\# completeness table after preprocessing jobs
completeness_table_file: tests/data/completeness_table.csv
\\
\# Path to the file defining the source model.
source_model_file: tests/data/an_input_area_source_model.xml
\\
\# Path to the file defining the results
\# of computation.
result_file: tests/data/an_input_area_source_model.xml
\\
\# Boolean flag to declare
\# if processing jobs are needed.
apply_processing_jobs: yes
\\
\# =========================================================
\# List of preprocessing jobs
\# =========================================================
\\
\# Choose one algorithm per preprocessing job,
\# algorithms will be executed in the specified
\# order.
preprocessing_jobs:
- GardnerKnopoff
- Stepp
\\
\# =========================================================
\# List of processing jobs
\# =========================================================
\\
\# Choose one algorithm per preprocessing job,
\# algorithms will be executed in the specified
\# order.
processing_jobs:
- Recurrence
- MaximumMagnitude
\\
\end{Verbatim}

\emph{...continues overleaf}

\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize, samepage=true]
\# =========================================================
\# Preprocessing jobs in detail
\# =========================================================
\\
\# Declustering jobs
\\
        ...
\\
\# Completeness jobs
\\
        ...
\# =========================================================
\# Processing jobs in detail
\# =========================================================
\\
\# Recurrence job
\\
        ...
\\
\# Maximum Magnitude Job
\\
        ...
\end{Verbatim}

In the initial lines, the toolkit is told the path to look for the input earthquake catalogue (\verb=eq_catalog_file=); the path to which it should write the ''pre-processed catalogue'' (\verb=pprocessing_catalog_file=); the path to which it should write the completeness table, or from which it should read the completeness table if no completeness jobs are defined (\verb=completeness_table_file=); the path from which it should find the area source geometries (\verb=source_model_file=), formatted here as xml (\verb=source_model_file=), and the path to write the output area sources (\verb=result_file=). 

The ''pre-processing'' refers to the combined operations for declustering and/or completeness - i.e. those that remove from consideration all events that are not indicative of complete stationary seismicity. If the user wishes to simply execute those jobs and no others then they should set the value \verb=apply_processing_jobs: no=. This will cause the program to terminate after completion of the pre-processing tasks. If the user has specified a valid path for the \verb=pprocessing_catalog_file= the program will export the catalogue with foreshocks, aftershocks and incomplete magnitude events removed.

After completion of the ''pre-processing'' tasks, the toolkit will then undertake the ''processing'' tasks. These are the calculations to determine the parameters of the double-truncated \cite{GutenbergRichter1944} distribution (i.e. a-value, b-value, minimum magnitude and maximum magnitude). If no source model input is specified in the \verb=source_model_file=, the processing will return (to the screen) a single a-value, b-value, $m_max$ and their respective uncertainties for the whole catalogue. Otherwise the program will loop over each of the zones specified in the file, select those earthquakes from the pre-processed catalogue that are found inside each zone and calculate the recurrence parameters for each zone. The output will then be written to the \verb=result_file=.

\textbf{The order in which the ''pre-processing'' and the ''processing'' tasks are define in the configuration is the order in which the program will execute them!}

\subsection{The Catalogue Format}

The input catalogue must be formatted as a comma-separated value file (.csv), with the following attributes (attributes with an * indicate essential attributes):

\begin{table}[h]
\begin{tabular}{|l|l|}  \hline 
Attribute & Description \\ \hline
eventID* & A unique identifier (integer) for each earthquake in the catalogue \\
Agency & The code (string) of the recording agency for the event solution  \\
Identifier & A secondary identifier (integer) not used at present  \\
year* & Year of event (integer) in the range -10000 to present \\
 & (events before common era (BCE) should have a negative value)\\
month* & Month of event (integer)\\
day* & Day of event (integer) \\
hour* & Hour of event (integer) - if unknown then set to 0 \\
minute* & Minute of event (integer) - if unknown then set to 0 \\
second* & Second of event (float) - if unknown set to 0.0 \\
timeError & Error in event time (float) \\
longitude* & Longitude of event, in decimal degrees (float) \\
latitude* & Latitude of event, in decimal degrees (float) \\
SemiMajor90 & Length (km) of the semi-major axis of the 90 \% \\
            & confidence ellipsoid for location error (float) \\
SemiMinor90 & Length (km) of the semi-minor axis of the 90 \% \\
            & confidence ellipsoid for location error (float) \\
ErrorStrike & Azimuth (in degrees) of the 90 \% \\
            & confidence ellipsoid for location error (float) \\
depth* & Depth (km) of earthquake (float)\\
depthError & Uncertainty (as standard deviation) in earthquake depth (km) (float)\\
Mw* & Moment magnitude of event (float) \\
sigmaMw & Uncertainty (standard deviation) in moment magnitude of event (float) \\
Ms & Surface-wave magnitude of event (float)\\
sigmaMs & Uncertainty (standard deviation) in surface-wave magnitude of event (float)  \\
mb & Body-wave magnitude of event (float)\\
sigmamb & Uncertainty (standard deviation) in body-wave magnitude of event (float) \\
ML & Local magnitude of event (float)\\
sigmaML & Uncertainty (standard deviation) in local magnitude of event (float) \\ \hline
\end{tabular}
\caption{List of Attributes in the Earthquake Catalogue File (* Indicates Essential)}
\label{tab: EQCatalogueFormat}
\end{table}

\subsection{The Source Model Format}

The source model typology currently supported by the Modeller's Toolkit is only the area source model, for which the source geometry is defined via a polygon. The source model is both input and output in GEM's NRML (\verb=Natural Risk Markup Language=) format (although support for shapefile input definitions will be forthcoming in future releases). This is due to the incremental development of the tool. At present there are no tools for defining other properties such as the rupture depth distribution or focal mechanism, so these must be defined independently of the analysis. As more features become available the need to setup the models in this manner will hopefully be relaxed further. A comprehensive description of the seismogenic source model input format required for OpenQuake is found in the OpenQuake book and manual, available online at \href{http://openquake.org/users/}{http://openquake.org/users/}, but the general format is given here:
 
\begin{Verbatim}[frame=single, commandchars=\\\{\},fontsize=\normalsize, samepage=true]
<\textcolor{red}{areaSource} gml:id="ID">
	<gml:name>NAME</gml:name>
	<tectonicRegion>TECT_REG_TYPE</tectonicRegion>
	<\textcolor{green}{areaBoundary}>
		...
	</\textcolor{green}{areaBoundary}>
	<textcolor{blue}{ruptureRateModel}>
		...
	</\textcolor{blue}{ruptureRateModel}>
	<\textcolor{magenta}{ruptureDepthDistribution}>
		...
	</\textcolor{magenta}{ruptureDepthDistribution}>
	<\textcolor{orange}{hypocentralDepth}>
		...	
	</\textcolor{orange}{hypocentralDepth}>
</\textcolor{red}{areaSource}>
\end{Verbatim}

As the intention of this model is to calculate the \cite{GutenbergRichter1944} a-value, b-value and $m_{max}$ the \verb=ruptureRateModel= must be defined as \verb=truncatedGutenbergRichter=, e.g.:

\begin{Verbatim}[frame=single, commandchars=\\\{\},fontsize=\normalsize, samepage=true]
<\textcolor{red}{truncatedGutenbergRichter}>
	<\textcolor{green}{aValueCumulative}>CUMULATIVE_A_VALUE</\textcolor{green}{aValueCumulative}>
	<\textcolor{blue}{bValue}>B_VALUE</\textcolor{blue}{bValue}>
	<\textcolor{magenta}{minMagnitude}>MINIMUM_MAGNITUDE</\textcolor{magenta}{minMagnitude}>
	<\textcolor{orange}{maxMagnitude}>MAXIMUM_MAGNITUDE</\textcolor{orange}{maxMagnitude}>
</\textcolor{red}{truncatedGutenbergRichter}>
\end{Verbatim}

As a default, initial parameters need to be entered into these fields prior to the analysis. It is therefore recommended to set these values to 0.0. 


\section{Declustering}

To identify Poissonian rate of seismicity, it is necessary to remove foreshocks/aftershocks/swarms from the catalogue. The Modeller's Toolkit contains, at present, two algorithms to undertakte this task, with more under development.

\subsection{\cite{GardnerKnopoff1974}}

The most widely applied simple windowing algorithm is that of \cite{GardnerKnopoff1974}. Originally conceived for Southern California, the method simply identifies aftershocks by virtue of fixed time-distance windows proportional to the magnitude of the main shock. Whilst this premise is relatively simple, the manner in which the windows are applied can be ambiguous. Four different possibilities can be considered (\cite{LuenStark2012}):

\begin{enumerate}
 
\item Search events in magnitude-descending order. Remove events if it is in the window of the largest event

\item Remove every event that is inside the window of a previous event, including larger events

\item An event is in a cluster if, and only if, it is in the window of at least one other event in the cluster. In every cluster remove all events except the largest

\item In chronological order, if the $i^{th}$ event is in the window of a preceding larger shock that has not already been deleted, remove it. If a larger shock is in the window of the $i^{th}$ event, delete the $i^{th}$ event. Otherwise retain the $i^{th}$ event.

\end{enumerate}

It is the first of the four options that is implemented in the current toolkit, whilst others may be considered in future.  The algorithm is capable if identifying foreshocks and aftershocks, simply by applying the windows forward and backward in time from the mainshock. No distinction is made between primary aftershocks (those resulting from the mainshock) and secondary or tertiary aftershocks (those originating due to the previous aftershocks); however, it is assumed all would occur within the window.

Several modifications to the time and distance windows have been suggested, which are summarised in \cite{vanStiphout2010}. The windows originally suggested by \cite{GardnerKnopoff1974} are approximated by:

\begin{equation} 
\mbox{distance (km)} = 10^{0.1238 M + 0.983}  \quad \mbox{time} = \begin{cases} 10^{0.032 M + 2.7389} & \text{if $M \geq 6.5$} \\ 10^{0.5409 M - 0.547} & \mbox{otherwise}  \end{cases}
\end{equation}

An alternative formulation is proposed by Gr\:unthal \cite{vanStiphout2010}:

\begin{equation} 
\mbox{distance (km)} = e^{1.77 + \left( {0.037 + 1.02 M} \right)^2}  \quad \mbox{time} = \begin{cases}   |e^{-3.95+ \left( {0.62 + 17.32 M} \right)^2}    & \text{if $M < 6.5$ } \\ 10^{2.8 + 0.024 M} & \text{otherwise}  \end{cases}
\end{equation}
A further alternative is suggested by \cite{Uhrhammer1986}
\begin{equation}
\mbox{distance (km)} = e^{-1.024 + 0.804 M} \quad \mbox{time} = e^{-2.87 + 1.235 M}
\end{equation}

A comparison of the expected window sizes with magnitude are shown for distance  and time (Figure \ref{fig:declust_scaling}).

\begin{figure}[htb]
  \centering
  \begin{subfigure}
      \centering
      \includegraphics[width=8cm]{./figures/declustering_distance_windows.eps}
%      \caption{Distance (km)}
%      \label{fig:declust_wind_dist}
	\end{subfigure}
	
  \begin{subfigure}
      \centering
      \includegraphics[width=8cm]{./figures/declustering_time_windows.eps}
%      \caption{Time}
%      \label{fig:declust_wind_time}
	\end{subfigure}	
	\caption{Scaling of declustering time and distance windows with magnitude}
	\label{fig:declust_scaling}
\end{figure}

The \cite{GardnerKnopoff1974} algorithm and its derivatives represent are most computationally straightforward approach to declustering. Whilst the windows themselves may be derived from judgement, the algorithm has been found to be surprisingly robust, with the resulting declustered catalogues often demonstrated to be Poissonian \cite{vanStiphout2010}. 

The \cite{GardnerKnopoff1974} algorithm in the Modeller's Toolkit is configured according to the following command:

\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
GardnerKnopoff: \{ \\
  \# Possible values: GardnerKnopoff, Uhrhammer, Gruenthal\\
  time_dist_windows: GardnerKnopoff,\\
\\
  \# float >= 0 proportion of aftershock time windows\\ 
  \# to use to search for foreshock.\\
  foreshock_time_window: 0\\
\}
\end{Verbatim}

The \verb=time_dist_windows= attribute indicates the choice of the time and distance window scaling model from the three listed. As the current version of this algorithm considers the events in a descending-magnitude order, the parameter \verb=foreshock_time_window= defines the size of the time window used for searching for foreshocks, as a fractional proportion of the size of the aftershock window (the distance windows are always equal for both fore- and aftershocks). So for an evenly sized time window for foreshocks and aftershocks, \verb=foreshock_time_window= should equal 1. For shorter or longer foreshock time windows this parameter can be reduced or increased respectively.

%\section{Reasenberg (1985)} – Code written, not implemented in MTK

\subsection{AFTRAN (\cite{Musson1999PSHABalkan})}

A particular development of the standard windowing approach is introduced in the program AFTERAN \cite{Musson1999PSHABalkan}. This is a modification of the \cite{GardnerKnopoff1974} algorithm, using a moving time window rather than a fixed time window. In AFTERAN, considering each earthquake in order of descending magnitude, events within a fixed distance window are identified (the distance window being those suggested previously). These events are searched using a moving time window of 100 days. For a given mainshock, non Poissonian events are identified if they occur both within the distance window and the initial time window. The time window is then moved, beginning at the last flagged event, and the process repeated. For a given mainshock, all non-Poissonian events are identified when the algorithm finds a continuous period of 100 days in which no aftershock or foreshock is identified. 

The theory of the AFTERAN algorithm is broadly consistent with that of \cite{GardnerKnopoff1974}. This algorithm, whilst a little more computationally complex, and therefore slower, than the \cite{GardnerKnopoff1974} windowing approach, remains simple to implement. 

The AFTERAN algorithm in the Modeller's Toolkit is configured according to the following:

\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
Afteran: \{\\
    \# Possible values: GardnerKnopoff, Uhrhammer, Gruenthal.\\
    time_dist_windows: Uhrhammer,\\
    \\
    \# float >= 0 \\
    \# Length (in days) of moving time window\\
    time_window: 60.0\\
\}
\end{Verbatim}

As with the \cite{GardnerKnopoff1974} function, the \verb=time_dist_windows= attribute indicates the choice of the time and distance window scaling model. The parameter \verb=time_window= indicates the size (in days) of the moving time window used to identify fore- and aftershocks. 

%::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\section{Completeness}

In the earliest stages of processing an instrumental seismic catalogue to derive inputs for seismic hazard analysis, it is necessary to determine the magnitude completeness threshold of the catalogue. To outline the meaning of the term ''magnitude completeness'' and the requirements for its analysis as an input to PSHA, the terminology of \cite{Mignan2010} is adopted. This defines the magnitude of completeness as the ''lowest magnitude at which 100 \% of the events in a space-time volume are detected (\cite{RydelekSacks1989, WoessnerWiemer2005})''. Incompleteness of an earthquake catalogue will produce bias when determining models of earthquake recurrence, which may have a significant impact on the estimation of hazard at a site. Identification of the completeness magnitude of an earthquake catalogue is therefore a clear requirement for the processing of input data for seismic hazard analysis.

It should be noted that this summary of methodologies for estimating completeness is directed toward techniques that can be applied to a ''typical'' instrumental seismic catalogue. We therefore make the assumption that the input data will contain basic information for each earthquake such as time, location, magnitude. We do not make the assumption that network-specific or station-specific properties (e.g., configuration, phase picks, attenuation factors) are known a priori. This limits the selection of methodologies to those classed as estimators of ''sample completeness'', which defines completeness on the basis of the statistical properties of the earthquake catalogue, rather than ''probability-based completeness'', which defines the probability of detection given knowledge of the properties of the seismic network (\cite{SchorlemmerWoessner2008}). This therefore excludes the methodology of (\cite{SchorlemmerWoessner2008}), and similar approaches such as that of \cite{Felzer2008}

The current workflows assume that completeness will be applied to the whole catalogue, ideally returning a table of time-varying completeness. The option to explore spatial variation in completeness is not explicitly supported, but could be accommodated by an appropriate configuration of the toolkit.

\subsection{User-defined Table}

This is simply a filtering that will remove from further consideration any events outside of the completeness bounds defined by the user. The table represents the time variation in $M_C$ and can be input as a separate file (in comma-separated value format) in the following format.
\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
1990.0, 4.0\\
1960.0, 5.0\\
1900.0, 6.0\\
1700.0, 7.0\\
\end{Verbatim}

The left-hand column represents the earliest year at which the earthquake is complete at the corresponding magnitude in the right-hand column. \\
\textbf{Important: The values in the completeness file must be entered from most-recent to oldest!} 

\subsection{\cite{Stepp1971}}

This is one of the earliest analytical approaches to estimation of completeness magnitude. It is based on estimators of the mean rate of recurrence of earthquakes within given magnitude and time ranges, identifying the completeness magnitude when the observed rate of earthquakes above MC begins to deviate from the expected rate. If a time interval ($T_i$) is taken, and the earthquake sequence assumed Poissonian, then the unbiased estimate of the mean rate of events per unit time interval of a given sample is:

\begin{equation}
   \lambda = \frac{1}{n} \sum_{i = 1}^{n} T_i
\end{equation}

with variance $\sigma_{\lambda}^{2} = \lambda / n$. Taking the unit time interval to be 1 year, the standard deviation of the estimate of the mean is:

\begin{equation}
   \sigma_{\lambda} = \sqrt{\lambda} / \sqrt{T}
\end{equation}

where $T$ is the sample length. As the Poisson assumption implies a stationary process, $\sigma_{\lambda}$ behaves as $1/\sqrt{T}$ in the sub-interval of the sample in which the mean rate of occurrence of a magnitude class is constant. Time variation of $M_C$ can usually be inferred graphically from Stepp's [1971] analysis, as is illustrated in Figure {fig: SteppFigExample1}. In this example, the deviation from the $1/sqrt{T}$ line for each magnitude class occurs at around 40 years  for $4.5 < M < 5$, 100 years for $5.0  < M < 6.0$, approximately 150 years for $6.0 < M < 6.5$ and 300 years for $M > 6.5$. Knowledge of the sources of earthquake information for a given catalogue may usually be reconciled with the completeness time intervals.

\begin{figure}[htbp]
	\centering
		\includegraphics[height=10cm, keepaspectratio=true]{./figures/C2Fig1SteppFig1.eps}
	\caption{Example of Completeness Estimation by the \cite{Stepp1971} methodology}
	\label{fig:SteppFigExample1}
\end{figure}

The analysis of \citet{Stepp1971} is a coarse, but relatively robust, approach to estimating the temporal variation in completeness of a catalogue. It has been widely applied since its development. The accuracy of the completeness magnitude depends on the magnitude and time intervals considered, and a degree of judgement is often needed to determine the time at which the rate deviates from the expected values. It has tended to be applied to catalogues on a large scale, and for relatively higher completeness magnitudes. 

To translate the methodology from a largely graphical methods into a computational method the completeness period needs to be identified by automatically identifying the point at which the gradient of the observed values decreases with respect to that expected from a Poisson process. This divergence is identified by the second derivative of the line. Estimation of the precise time, however, can depend on what the user defines as an acceptable degree of divergence. To accommodate this, we define the parameter \verb=sensitivity=, which corresponds to a dimensionless value describing the degree of divergence. In most cases values between 0.05 and 0.2 are recommended.

In the current version of the Modeller's Toolkit only the \cite{Stepp1971} methodology for analysis of catalogue completeness is implemented. 

\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
Stepp: \{\\
  \# Time Window of each step (in years)\\
  time_window: 5,\\
\\
  \# Magnitude window of each step (in Mw units)\\
  magnitude_windows: 0.2,\\
  \\
  \# Sensitivity parameter (see documentation)\\
  sensitivity: 0.1,\\
  \\
  \# Increment Lock (fixes that the completeness magnitude\\
  \# will always increase further back in time)\\
  increment_lock: True \\
\} 
\end{Verbatim}

The \verb=time_window= parameter describes the size of the time window in years, the \verb=magnitude_window= parameter describes the size of the magnitude bin, sensitivity is as described previously. The final option (\verb=increment_lock=) is an option that is used to ensure consistency in the results to avoid the completeness magnitude increasing for the latest intervals in the catalogue simply due to the variability associated with the short duration. If \verb=increment_lock= is set to \verb=True=, the program will ensure that the completeness magnitude for shorter, more recent windows is less than or equal to that of older, longer windows. Otherwise it should be set to \verb=False to show the apparent variability=. Some degree of judgement is necessary here. In particular it is expected that the user may be aware of circumstances particular to their catalogue for which a recent increase in completeness magnitude is expected (for example, a certain recording network no longer operational).  


%::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

\section{Recurrence Models}

The current sets of tools are intended to determine the parameters of the 
\subsection{Maximum Likelihood}

The classical maximum likelihood estimator for a simple unbounded \cite{GutenbergRichter1944} model is that of \cite{Aki1965}, adapted for binned magnitude data by \cite{Bender1983}. It assumes a fixed completeness magnitude ($M_C$) for the catalogue, and a simple power law recurrence model. It does not explicitly take into account magnitude uncertainty.


\begin{equation}
   b = \frac{ \log_{10} \left( e \right)}{ \bar{m} - m_0 + \left( {\frac{\Delta M}{2}} \right)}
\end{equation}

 We adjust for time-variation in completeness by divide the catalogue into S sub-catalogues, where each sub-catalogue corresponds to a period with a corresponding $M_C$. An “average” a- and b-value (with uncertainty) is returned by taking the mean of the a- and b-value of each sub-catalogue, weighted by the number of events in each sub-catalogue.

\begin{equation}
   \hat{b} = \frac{1}{S} \sum_{i = 1}^{S} w_i b_i
\end{equation}

\subsection{\cite{Weichert1980}}

Recognising the typical conditions of an earthquake catalogue, \cite{Weichert1980} developed a maximum likelihood estimator of $b$ for grouped magnitudes and unequal periods of observation. The likelihood formulation for this approach is:

\begin{equation}
   \mathbf{L} \left( {\beta | n_i, m_i, t_i} \right) = \frac{ N!}{\prod_i n_i!} \prod_i p_{i}^{n_i}
\end{equation}

where $\mathbf{L}$ is the likelihood estimator of $\beta$, $n$ the number of earthquakes in magnitude bin m with observation period t. The parameter $p$ is defined as:

\begin{equation}
   p_i = \frac{t_i \exp \left( {-\beta m_i} \right) }{\sum_j t_j \exp \left( {-\beta m_j} \right)}
\end{equation}

The extremum of $\ln \left( {\mathbf{L}}\right)$ is found at:

\begin{equation} 
   \frac{\sum_i t_i m_i \exp \left( {-\beta m_i} \right)}{\sum_j t_j \exp \left( {-\beta m_j} \right)}
\end{equation}

The computational implementation of this method is given as an appendix to \cite{Weichert1980}. This formulation of the maximum likelihood estimator for b-value, and consequently seismicity rate, is in widespread use, with applications in many national seismic hazard analysis \citep[e.g.]{usgsNSHM1996,usgsNSHM2002}. The algorithm has been demonstrated to be efficient and unbiased for most applications. It is recognised by \citet{Felzer2008} that an implicit assumption is made regarding the stationarity of the seismicity for all the time periods. 

To implement either of the recurrence estimators described here the configuration properties are defined as such:

\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
Recurrence: \{\\
    \# Width of magnitude window positive float\\
    magnitude_window: 0.2,\\
\\  
    \# Choose one among Weichert or MLE\\
    recurrence_algorithm: Weichert,\\
\\
    \# A float\\
    reference_magnitude: 5.0,\\
\\
    \# Greater than zero (float), used only with Weichert\\
    time_window: 1.0\\
\}
\end{Verbatim}

Where \verb=magnitude_window= indicates the size of the magnitude bin, \verb=recurrence_algorithm= the choice of the algorithm to use (either \verb=Weichert= or \verb=MLE=) and \verb=reference_magnitude= the magnitude for which the output calculates that rate greater than or equal to (set to \verb=0= for $10^{a}$). The \verb=time_window= indicates the bin of the time window used in the \cite{Weichert1980} algorithm. This value should generally be set to 1.0 for stability (the option to change it will be removed in future versions).


%::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
\section{Maximum Magnitude}

The estimation of the maximum magnitude for use in seismic hazard analysis is a complex, and often controversial, process that should be guided by information from geology and the seismotectonics of a seismic source. Estimation of maximim magnitude from the observed (instrumental and historical) seismicity can be undertaken using the following non-parametric methods. These methods have been chosen from the possible ones available as they are intended to be independent of an assumption of the functional form of the recurrence model, and are therefore not conditioned upon b-value. 

\subsection{\cite{Kijko2004}}

Three different estimators of maximum magnitude are given by \cite{Kijko2004}, each depending on a different set of assumptions:
\begin{enumerate}
\item ''Fixed b-value'': Assumes a single b-value with no uncertainty (not considered here)
\item ''Uncertain b-value'': Assumes and uncertain b-value defined by an expected b and the standard deviation (not considered here)
\item ''Non-Parametric Gaussian'': Assumes no functional form (can be applied to seismicity observed to follow a more characteristic distribution)
\end{enumerate}

Whilst the fixed and uncertain b-value estimators can be applied in many situations, at present only the ''Non-Parametric Gaussian'' estimator is implemented here (the remainder will be implemented in due course).

The non-parametric Gaussian estimator for maximum magnitude $m_{max}$ is defined as:
%\begin{equation}
\begin{eqnarray}
m_{max} & = & m_{max}^{obs} + \Delta \\
\Delta  & = & \int\limits_{m_{min}}^{m_{max}} \left[ {\frac{\sum_{i = 1}^{n} \left[ {\Phi \left( {\frac{m - m_i}{h}} \right) - \Phi \left( {\frac{m_{min} - m_i}{h}} \right)} \right]}{\sum_{i = 1}^{n} \left[ {\Phi \left( {\frac{m_{max} - m_i}{h}} \right) - \Phi \left( {\frac{m_{min} - m_i}{h}} \right)} \right]}} \right]^n  dm
\end{eqnarray}
%\end{equation}
where $m_{min}$ and $m_{max}$ are the minimum and maximum magnitudes from a set of $n$ events, $\Phi$ is the standard normal cumulative distribution function. $h$ a kernel smoothing factor:
\begin{equation}
h = 0.9 \times min\left( {\sigma, IQR / 1.34} \right) \times n^{-1 / 5}
\end{equation}
with $\sigma$ the standard deviation of a set of n earthquakes with magnitude $m_{i}$ where $i = 1, 2, ... n$, and $IQR$ the inter-quartile range. 

The uncertainty on maximum magnitude is determined via:

\begin{equation}
    \sigma_{m_{max}} = \sqrt{\sigma_{m_{max}^{obs}}^2 + \Delta^{2}}
\end{equation}

Therefore the uncertainty on $m_{max}$ is conditioned primarily on the uncertainty of the largest observed magnitude. As in many catalogues the largest observed magnitude may be an earlier historical event, which will be associated with a large uncertainty, this estimator tends towards large uncertainties on $m_{max}$.

%\section{EPRI (1994), CEUS 2012} - Investigating
\subsection{Cumulative Moment\citep{MakropoulosBurton1983}}

The cumulative moment release method is an adaptation of the cumulative strain energy release method for estimating $m_{max}$ originally proposed by \cite{MakropoulosBurton1983}. Another method based on a pseudo-graphical formulation, an estimator of maximum magnitude can be derived from a plot of cumulative seismic moment release with time. The average slope of this plot indicates the mean moment release for the input catalogue in question. Two further straight lines are defined with gradients equal to that of the slope of mean cumulative moment release, both enveloping the cumulative plot. The vertical distance between these two lines indicates the total amount of moment that may be released in the region, if no earthquakes were to occur in the corresponding time (i.e. the distance between the upper and lower bounding lines on the time axis). This concept is illustrated in Figure \ref{fig:Cumulative_Moment}. 

\begin{figure}[htb]
	\centering
		\includegraphics[height=6cm, keepaspectratio=true]{./figures/Cumulative_Moment.eps}
	\caption{Illustratation of Cumulative Moment Release Concept}
	\label{fig:Cumulative_Moment}
\end{figure}

The cumulative moment estimator of $m_{max}$, whilst simple in concept, has several key advantages. As a non-parametric method it is independent of any assumed probability distribution and cannot estimate $m_{max}$ lower than the observed $m_{max}$. It is also principally controlled by the largest events in the catalogue, this making it relative insensitive to uncertainties in completeness or lower bound threshold. In practice, this estimator, and to some extent that of \cite{Kijko2004} are dependent on having a sufficiently long record of events relative to the strain cycle for the region in question, such that the estimate of average moment release is stable. This will obviously depend on the length of the catalogue, and for some regions, particularly those in low strain intraplate environments, it is often the case that $m_{max}$ will be close to the observed $m_{max}$. Therefore it may be the case that it is most appropriate to use these techniques on a larger scale, either considering multiple sources or an appropriate tectonic proxy.

For the cumulative moment estimator it is possible to take into account the uncertainty on $m_{max}$ by applying bootstrap sampling to the observed magnitudes and their respective uncertainties. This has the advantage that $\sigma_{m_{max}}$ is not controlled by the uncertainty on the observed $m_{max}$, as it is for the \cite{Kijko2004} algorithm. Instead it takes into account the uncertainty on all the magnitudes in the catalogue. The cost of this, however, is that this method is more computationally intensive, and therefore slower, than \cite{Kijko2004}, depending on the number of bootstrap samples the user chooses.

The maximum magnitude algorithm is configured according to the following settings in the ''processing'' jobs: 


\begin{Verbatim}[frame=single, commandchars=\\\{\}, fontsize=\scriptsize]
MaximumMagnitude: \{\\
    \# Choose one among `Kijko_Npg`, `Cumulative_Moment`.\\
    \# default choice: Cumulative_Moment,\\
    maxim_mag_algorithm: Kijko_Npg,\\
\\
    \# float > 0, default value 1.0E-5\\
    iteration_tolerance: 1.0E-5,\\
\\
    \# int > 0, default value 1000\\
    maximum_iterations: 1000,\\
\\
    \# neq\\
    neq: 100,\\
\\
    \# Used in Kijko_Npg\\
    \# int > 0 default value 51\\
    number_samples: 51,\\
\\
    \# Used in Cumulative_Moment\\
    \# int > 0 default value 100\\
    number_bootstraps: 100\\
\}
\end{Verbatim}

The \verb=maxim_mag_algorithm= indicates the choice of algorithm used, \verb=iteration_tolerance= and \verb=maximum_iterations= are parameters controlling the convergence of the iteration (only applicable to \cite{Kijko2004}), \verb=neq= corresponds to the number of largest earthquakes used for the \cite{Kijko2004} algorithm and \verb=number_samples= the number of samples of the cumulative Gaussian distribution function used inside the integral of the \cite{Kijko2004} function. More samples increases accuracy, but at the cost of speed. For the cumulative moment algorithm the only user configurable parameter is the \verb=number_bootstraps=, which is the number of samples used during the bootstrapping process. 